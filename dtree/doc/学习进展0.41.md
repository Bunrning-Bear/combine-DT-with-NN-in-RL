<center>

### **使用完全随机树和神经网络算法进行集成学习**

陈雄辉 

 Version 0.41

2017.6.3 

| Date      | Change log                               | version |
| --------- | ---------------------------------------- | ------- |
| 2017.5.19 | 完成第一个版本                                  | 0.1     |
| 2017.5.21 | 考虑增量学习，对决策树的部分逻辑进行重写，将原本的NN 换成了Learn++算法 | 0.2     |
| 2017.6.1  | 对0.2版本的一些语句表述做微调                         | 0.21    |
| 2017.6.3  | 保留增量学习的大框架，但是实际上还是用NN来进行重新训练，删除Learn++   | 0.3     |
| 2017.6.3  | 调整决策树相关部分的业务逻辑，将遍历每一颗决策树的逻辑抽离到main中，方便之后的迭代拓展 | 0.4     |
| 2017.6.3  | 删除多颗决策树的逻辑，做单颗决策树的第一个迭代版本                | 0.41    |

</center>

#### **版本说明**：

> 1. [0.4] 把Distribution, Classifier driver, Training Driver 三个部分的遍历决策树的逻辑抽出到main中
> 2. [0.41]删除分类驱动器，0.40版本统一抽出的遍历多个决策树相关逻辑暂时删除，先只做一颗决策树的实现
> 3. 文档是在原来的基础上迭代的，主要的更新的地方我用了[update]标签，新增的部分我用了[new]标注，不带标注的就是没有经过主要修改的。方便老师阅读。
>

<div style="page-break-after: always;"></div>

#### **1. [update]伪代码**

注意：下面代码使用的是python风格的伪代码，**其中“#”代表的是注释的内容**

##### **Variable**

> 伪代码涉及的全局变量

- Training set : $S= \{S_1,S_2,..,S_K\}$,一共有K个数据集,对于任意一个数据集$S_i =\{(x_1,t_1),...,(x_n,t_n)\}$，一共n个样本
- Features: $X=\{F_1,F_2,...,F_k\}$，一共k个特征
- Random trees: $T =\{T_1,T_2...,T_N\}$,一共有N颗树
- MAX_DEPTH = $\frac{k}{2}$，每棵树的最大深度，根据论文所证明的，该深度能够保证随机树的多样性
- [保留，暂时没用上]MIN_BATCH = 100，增量更新的最小样本个数

##### **Preprocessor**

> 预处理

- 清洗：若用于检验算法的训练集有非法数据直接清除，**实际使用时不考虑清洗**
- 编写预处理器，将数据集转化为后续函数可以直接调用的标准数据集格式

##### **Decision trees creator**

> 构造单颗决策树：

BuildTreeStructure($T$,$X$) **# 其中$T$为树的实例对象[提醒：在树的结构中，每一个节点都是一颗树]，$X$为特征集合**

begin

​	if $X$ == EMPTY :  **# 当前的树已经没有特征可供划分**

​		将当前的$T$作为叶子节点

​	else if T.depth > MAX_DEPTH: **# 已经超过了允许的最大深度，停止树的构造，【change】但是树结构最大允许5层**

​		将当前的$T$作为叶子节点

​	else **# 随机选择特征，递归构造子树**

​		从特征集合$X$中随机选择一个特征$F$，如果是连续属性，则**从当前样本集对于该特征的取值区间中**，随机选取一个为基准进行划分

​		获取$F$的所有观察值，记为集合{$d_j$}，假设$F$一共有$m$个观察值

​		对于当前的树$T$,对每一个观察值，生成一个子节点$n_j$,设一共有$m$个子节点

​		for j = 1:m do**# 对每一个节点递归构造子树**

​			BuildTreeStructure($n_j$,$X-\{F\}$);

​		end

​	end		

end				

##### **[update]Distribution**

> 将样本数据分发到各个节点并记录，用于之后的增量学习(即Incremental Learning Driver 函数中)，做到每一个样本的流式处理

Distribution($T,(x,t)$) # $T$ 是一颗决策树，($x$,$t$)是当前分发的样本

begin 

​	if $T$ 不是叶子节点:

​		假设$d$是样本$x$在当前节点选择的特征$F$的观察值，也就是 $d=F(x)$，

​		求得$T$与$d$对应的子节点$n$

​		Distribution($n,(x,t)$) 

​	else

​		**# 递归退出条件：T是叶子节点**

​		将该样本的索引下标添加到在当前节点$T$->data_index

​	end

end

##### **[update]Incremental Training Driver **

>  对每一个叶子节点构造一个神经网络，这个部分只是用于调用每个节点网络进行增量训练的驱动器，调用该叶子节点的网络模型，进行训练。

Training_Driver($T$,$S$):

​	begin

​		得到$T$ 的叶子节点的集合$\{n_j\}$，设一共有m个叶子节点，记录该节点的公共特征$\{F_j\}$

​		for j = 1:m 			

​			根据$n_j$->data_index 的获取该节点的样本 集合${S_j}$，剔除样本中的公共特征$\{F_j\}$,也就是只保留特征$X-\{F_j\}$  



​			使用$S_j$ 带入$n_j$->model->Learning($S_j$ ) 进行增量学习

​			学习完成后，将 $n_j$->data_index  归零 

​		end

​	end

[change] (实际实现中，我们直接记录了保留特征)

##### **Incremental Learning**

>  每个神经网络的节点，对于新来的数据而言，我们做的是在该网络原来权重的基础上重新进行一遍梯度下降

$n$->model->Learning($S$) **# n是调用增量学习算法的节点，S用于训练的数据集**

​	# 进行神经网络的初始化

​	w = $n$->model->w

​	$n$->model->initial(w)

​	end

##### **[update] Classifier Driver**

> 分类驱动器，用于最后做预测。这个部分是分类器的驱动，主要负责各个决策树的投票

Classifier_Driver($T,x$,vote) #$T$是一颗决策树的集合，$x$是待分类的样本

​	begin

​		**# 寻找该样本在该棵树下映射到哪个节点上**

​		while $T$ 不是 叶子节点

​			假设$d$是样本$x$在当前节点选择的特征$F$的观察值，也就是$d=F(x)$，

​			我们令求得$T_i$与$d$对应的子节点$n$，更新：$T_i=n$

​		end

​		使用$T_i$->model->Classifier($x$) 进行预测，得到分类结果为$t_i$

​		return $t_i$

​	end

##### **[update]Main function**

>  主函数，

main()

​	begin

​		 导入数据，设我们一共有K个数据集$S’= \{S_1,S_2,..,S_K\}$

​		进行数据预处理，得到标准化的数据集$S$

​		初始化单颗完全随机决策树$T $

​		初始化 $X=\{F_1,F_2,...,F_k\}$，一共k个特征，寻找样本$S_1$的每一个连续属性特征${F_n}$的max和min值，将结果记录于$X$中

​		# **构造完全随机决策树**

​		BuildTreeStructure($T$,$X$)

​		 for m = 1,..,K do			

​			n = size($S_m$) **# n为该数据集的样本个数**

 			for i= 1:n**#  对每个进行样本的分发**

​				对于所有的$(x_i,t_i) \in S_m$进行分发：

​				Distribution($T,(x_i,t_i)$)

​			end

​			Training_Driver($T$,$S_m$) **# 调用训练驱动器，进行训练**

​		end

​		input $x$

​		output Classifier_Driver($T$,$x$) **# 调用分类驱动器，进行预测**

​	end

#### **2. 数据集的选用**

本次训练的样本与《Is random model better? On its accuracy and efficiency 》一样，保证我们的算法优劣有参考标准可以看，我们采用以下两个数据集

- kddcup 98 dataset:http://kdd.ics.uci.edu/databases/kddcup98/kddcup98.html
- UCI adult dataset：http://archive.ics.uci.edu/ml/datasets/Adult

#### **3. 算法评价指标**

使用与《Is random model better? On its accuracy and efficiency 》一样的评价指标，也就是用benefit matrix进行评价。在原论文p5有提及，以下分别是两个选用数据集的benefit matrix：

- KDDCUP 98:

  <img src="data1.png" style="zoom:70%" />

- UCI adult:

  <img src="data2.png" style="zoom:70%" />

####　**4. 问题和后续改进**

##### **2017.5.19**

- **[question]**之前所说的是规定决策树的最大深度或最小样本数量，但是我发现这两者若同时使用，目前感觉并不会有什么问题，不知道是否可以同时使用?我认为是可以的。

  A: 可以。（但是如果用于增量学习，应该不存在最小样本数量和决策树的关系，最小样本数实际上影响的是该节点的网络是否更新

- **[question]**同时，关于最小样本数量，是使用预剪枝还是后剪枝的方式来进行处理，我做了一定的考虑。目前决定是进行后剪枝，主要考虑点是：如果进行预剪枝，就需要每次都判断当前的节点是否满足100个样本，是的话进一步遍历。这就需要反复读取样本数据信息来进行分类，因为数据集太大的情况下，有可能没法完全读入内存，因此需要大量的IO，个人认为这样的内存损耗大过每个样本只使用一次(流式处理)，最后再进行统一剪枝的损耗。不知道这样的考虑是否正确？

  A: 不要后剪枝，因为增量学习中，我们的样本是不断补充的

- **[question]**在随机树中，我们随机选择了特征$F$的子集$F'$那么我们在叶子节点训练神经网络时，到达叶子节点之后，我们已经有一部分的特征是固定的，我们设为$F_0$。那么，我们要使用${F'-F_0}$为全部的样本的全部特征，还是以${F-F_0}$为样本的全部特征？目前我倾向于前者，前者可以进一步降低特征的数目，而分类的准确度可以通过多个决策树的投票来弥补；后者，显然因为样本拥有更多的特征，所以每个神经网络模型分类准确度更高，但是随之带来的则是性能的损失。并且，由于我们最后有投票的过程，所以这部分的信息的训练有可能有点冗余。

  A: 使用${F-F_0}$为样本的全部特征，因为对于$F'$而言，所使用的特征有限，我们的决策树终究只是用来做初次分类的，不会用到太多特征。

- **[to improve]**我认为该算法中的训练过程是可以变并行算的，Training with NN过程 ，每一个节点都是独立训练，相互之间没有关系。但是我对ＧＰＵ的并行计算不太了解。如果是传统的ＣＰＵ的运算的话，直接开线程运算即可。



#### **5. 参考资料**

[1] Fan W, Wang H, Yu P S, et al. Is random model better? On its accuracy and efficiency[C]//Data Mining, 2003. ICDM 2003. Third IEEE International Conference on. IEEE, 2003: 51-58.

http://ieeexplore.ieee.org/document/1250902/

[2] Polikar R, Upda L, Upda S S, et al. Learn++: An incremental learning algorithm for supervised neural networks[J]. IEEE transactions on systems, man, and cybernetics, part C (applications and reviews), 2001, 31(4): 497-508.

http://ieeexplore.ieee.org/abstract/document/983933/


